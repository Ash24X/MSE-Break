{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken transformers_stream_generator"
      ],
      "metadata": {
        "id": "Gqto-EIFMdH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "8mkpAQOnJZJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "S0bIxZUVJc1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qp49fdcNJKHt"
      },
      "outputs": [],
      "source": [
        "model_name = \"Qwen/Qwen-7B-Chat\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/narcotics_trial3.csv\", skiprows=1)\n",
        "refused = df[\"Refused Question\"]\n",
        "accepted = df[\"Completed Question\"]\n",
        "accepted_list = accepted.to_list()\n",
        "refused_list = refused.to_list()"
      ],
      "metadata": {
        "id": "1ni2QN3AJpqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_embeddings(word: str, sentences: list, tokenizer, model, layer):\n",
        "    embeddings = []\n",
        "    word_tokens = tokenizer.tokenize(word)\n",
        "    model.to(device)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if tokenizer.pad_token is None:\n",
        "          tokenizer.pad_token = tokenizer.eos_token\n",
        "          tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "        tokens = tokenizer(sentence, return_tensors=\"pt\", truncation=True).to(device)\n",
        "        input_ids = tokens[\"input_ids\"].squeeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**tokens, output_hidden_states=True)\n",
        "\n",
        "        token_embeddings = outputs.hidden_states[layer].squeeze(0)\n",
        "        token_strs = [tokenizer.convert_ids_to_tokens(token_id) for token_id in input_ids.tolist()]\n",
        "\n",
        "        indices = []\n",
        "        i = 0\n",
        "        while i < len(token_strs):\n",
        "            if any(word_token in token_strs[i] for word_token in word_tokens):\n",
        "                matched = True\n",
        "                for j in range(1, len(word_tokens)):\n",
        "                    if i + j >= len(token_strs) or word_tokens[j] not in token_strs[i + j]:\n",
        "                        matched = False\n",
        "                        break\n",
        "                if matched:\n",
        "                    indices.extend(range(i, i + len(word_tokens)))\n",
        "            i += 1\n",
        "\n",
        "        if indices:\n",
        "            word_embedding = token_embeddings[indices].mean(dim=0)\n",
        "            embeddings.append(word_embedding)\n",
        "        else:\n",
        "            print(sentence)\n",
        "            print(tokenizer.tokenize(sentence))\n",
        "\n",
        "    return torch.stack(embeddings) if embeddings else torch.empty(0, device=device)"
      ],
      "metadata": {
        "id": "r-Es0wPuJVU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pca_embeddings(accepted_embeddings, refusal_embeddings):\n",
        "    print(\"Accepted embeddings shape:\", accepted_embeddings.shape)\n",
        "    print(\"Refusal embeddings shape:\", refusal_embeddings.shape)\n",
        "\n",
        "    all_embeddings = torch.cat([accepted_embeddings, refusal_embeddings], dim=0).to(torch.float16)\n",
        "    print(\"All embeddings shape:\", all_embeddings.shape)\n",
        "\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_result = pca.fit_transform(all_embeddings.cpu().numpy())\n",
        "    print(\"PCA shape:\", pca_result.shape)\n",
        "\n",
        "    accepted_pca = pca_result[:len(accepted_embeddings)]\n",
        "    refusal_pca = pca_result[len(accepted_embeddings):]\n",
        "    print(\"Accepted PCA shape:\", accepted_pca.shape)\n",
        "    print(\"Refusal PCA shape:\", refusal_pca.shape)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    # Plot accepted points with their respective indices\n",
        "    plt.scatter(accepted_pca[:, 0], accepted_pca[:, 1], color=\"blue\", alpha=0.7, label=\"Accepted\")\n",
        "    #for i, (x, y) in enumerate(accepted_pca):\n",
        "        #plt.text(x, y, f\"A-{i}\", fontsize=8, color=\"blue\")\n",
        "\n",
        "    # Plot refusal points with their respective indices\n",
        "    plt.scatter(refusal_pca[:, 0], refusal_pca[:, 1], color=\"red\", alpha=0.7, label=\"Refusal\")\n",
        "    #for i, (x, y) in enumerate(refusal_pca):\n",
        "        #plt.text(x, y, f\"R-{i}\", fontsize=8, color=\"red\")\n",
        "\n",
        "    plt.title(\"PCA of Accepted vs. Refusal Embeddings\")\n",
        "    plt.xlabel(\"PCA Component 1\")\n",
        "    plt.ylabel(\"PCA Component 2\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "d7CMqLdfJ6Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0opK7SrcKFjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accepted_embeddings = get_word_embeddings(\" narcotics\", accepted_list, tokenizer, model, layer=15)\n",
        "print(accepted_embeddings.shape)\n",
        "\n",
        "refusal_embeddings = get_word_embeddings(\" narcotics\", refused_list, tokenizer, model, layer=15)\n",
        "print(refusal_embeddings.shape)"
      ],
      "metadata": {
        "id": "1211DWx6JP0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_pca_embeddings(accepted_embeddings, refusal_embeddings)"
      ],
      "metadata": {
        "id": "VGXheaZlKA2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [13,14,15,16,17]\n",
        "\n",
        "for layer in layers:\n",
        "    accepted_embeddings = get_word_embeddings(\" narcotics\", accepted_list, tokenizer, model, layer=layer)\n",
        "    refusal_embeddings = get_word_embeddings(\" narcotics\", refused_list, tokenizer, model, layer=layer)\n",
        "\n",
        "    print(\"LAYER NUMBER:\", layer)\n",
        "    plot_pca_embeddings(accepted_embeddings, refusal_embeddings)"
      ],
      "metadata": {
        "id": "X6gGxKrO2kEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing embeddings at certain positions"
      ],
      "metadata": {
        "id": "O_m0lHkOoZJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos_embeddings(sentences: list[str], tokenizer, model, layer, pos):\n",
        "  embeddings = []\n",
        "  model.to(device)\n",
        "\n",
        "  for sentence in sentences:\n",
        "      tokens = tokenizer(sentence, return_tensors=\"pt\", truncation=True).to(device)\n",
        "      with torch.no_grad():\n",
        "          outputs = model(**tokens, output_hidden_states=True)\n",
        "\n",
        "      hidden_state = outputs.hidden_states[layer][:, pos, :].squeeze(0)\n",
        "      embeddings.append(hidden_state)\n",
        "\n",
        "\n",
        "  return torch.stack(embeddings) if embeddings else torch.empty(0, device=device)"
      ],
      "metadata": {
        "id": "riG_HzNKoY2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [i for i in range(1, len(model.transformer.h))]\n",
        "pos=-3\n",
        "\n",
        "for layer in layers:\n",
        "    accepted_embeddings = get_pos_embeddings(accepted_list, tokenizer, model, layer=layer, pos=pos)\n",
        "    refusal_embeddings = get_pos_embeddings(refused_list, tokenizer, model, layer=layer, pos=pos)\n",
        "\n",
        "    print(\"LAYER NUMBER:\", layer)\n",
        "    plot_pca_embeddings(accepted_embeddings, refusal_embeddings)"
      ],
      "metadata": {
        "id": "_5XDty_5oltR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}